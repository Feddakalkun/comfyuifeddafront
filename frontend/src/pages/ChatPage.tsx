import { useState, useRef, useEffect } from 'react';
import { Send, Bot, User, Sparkles, Image as ImageIcon, Loader2, Type, Mic, Square, Volume2, VolumeX } from 'lucide-react';
import { assistantService } from '../services/assistantService';
import { comfyService } from '../services/comfyService';
import { ollamaService } from '../services/ollamaService';
import ReactMarkdown from 'react-markdown';

interface Message {
    id: string;
    role: 'user' | 'assistant';
    content: string;
    type?: 'text' | 'image-generation-request' | 'image-result';
    timestamp: number;
    metadata?: any;
    images?: string[]; // Base64 images for vision models
}

// System prompt to guide the Agent
// System prompt to guide the Agent
const AGENT_SYSTEM_PROMPT = `You are FEDDA AGENT, an Elite Creative Director and Expert Stable Diffusion Prompt Engineer.
Your goal is to help the user create AWARD-WINNING visuals via ComfyUI.

### ðŸ§  CORE DIRECTIVES:
1.  **Visual Excellence:** Never settle for boring. Always aim for "Z-Image" quality: Photorealistic, Cinematic, High Detail, 8k.
2.  **Context Mastery & Logic:** ALWAYS CHECK CHAT HISTORY. If the user says "change hair to blue", REMEMBER the previous image details (pose, setting) and ONLY change the hair. If the user requests a LOCATION change (e.g. "move her to a bus"), REMOVE old locations (e.g. "mountains") and replace them. Do not include conflicting settings in one prompt.
3.  **Vision Analysis:** If an image is uploaded, analyze its composition, lighting, and style. Use it as inspiration.

### ðŸŽ¨ PROMPT INGREDIENTS (Use freely to enhance prompts):
-   **Lighting:** Volumetric, Cinematic, Rembrandt, Bioluminescent, God Rays, Studio Softbox, Hard Rim Lighting.
-   **Camera:** 85mm Portrait, Macro, Wide Angle, Drone View, GoPro, Bokeh/Depth of Field, F/1.8.
-   **Details:** Skin pores, fabric texture, water droplets, dust particles, film grain, imperfect skin.
-   **Styles:** Cyberpunk, Fantasy, Noir, Vaporwave, Cinematic, Corporate, 1990s VHS, Analog Photography.

### ðŸš€ BEHAVIOR:
-   **Simple Request:** If user says "a cat", EXPAND it: "A majestic Maine Coon in a neon alley, rain, volumetric fog, cybernetic details."
-   **Specific Request:** If user is specific, FOLLOW EXACTLY.
-   **Conversation:** If user says "Hi" or chats, just reply nicely. DO NOT GENERATE.

### ðŸ“¢ FORMAT (Only when generating):
<<GENERATE>>
[Subject & Pose], [Clothing], [Environment/Background], [Lighting & Mood], [Camera & Angle], [Style Tags], [Tech Specs: best quality, 8k, masterpiece]
<</GENERATE>>`;

export const ChatPage = () => {
    const [messages, setMessages] = useState<Message[]>([
        {
            id: 'welcome',
            role: 'assistant',
            content: "Hey! I'm your creative AI assistant. I can help you brainstorm ideas, answer questions, and generate images when you need them. How can I help you?",
            timestamp: Date.now(),
            type: 'text'
        }
    ]);
    const [input, setInput] = useState('');
    const [isThinking, setIsThinking] = useState(false);
    const [generatingMsgId, setGeneratingMsgId] = useState<string | null>(null);
    const [availableModels, setAvailableModels] = useState<string[]>([]);
    const [selectedModel, setSelectedModel] = useState<string>('');
    const [availableLoras, setAvailableLoras] = useState<string[]>([]);
    const [selectedLora, setSelectedLora] = useState<string>('');
    const [executionStatus, setExecutionStatus] = useState('');
    const [progress, setProgress] = useState(0);
    const [fontSize, setFontSize] = useState<'small' | 'medium' | 'large'>('medium');
    const [isRecording, setIsRecording] = useState(false);
    const [isTranscribing, setIsTranscribing] = useState(false);
    const [recordingMode, setRecordingMode] = useState<'hold' | 'toggle'>('hold');

    // Drag & Drop / Vision State
    const [isDragging, setIsDragging] = useState(false);
    const [pendingImages, setPendingImages] = useState<string[]>([]); // Base64 strings

    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const audioChunksRef = useRef<Blob[]>([]);
    const recordingTimeoutRef = useRef<number | null>(null);

    // TTS State
    const [ttsEnabled, setTtsEnabled] = useState(false);
    const [playingMsgId, setPlayingMsgId] = useState<string | null>(null);
    const [generatingTtsId, setGeneratingTtsId] = useState<string | null>(null);
    const [voiceStyle, setVoiceStyle] = useState('female, clear voice');
    const audioPlayerRef = useRef<HTMLAudioElement | null>(null);

    const messagesEndRef = useRef<HTMLDivElement>(null);

    const fontSizeClasses = {
        small: 'text-sm',
        medium: 'text-base',
        large: 'text-lg'
    };

    // Fetch Models & LoRAs on Mount
    useEffect(() => {
        const fetchData = async () => {
            try {
                // Models
                const models = await ollamaService.getModels();
                if (models.length > 0) {
                    setAvailableModels(models.map(m => m.name));
                    const preferred = models.find(m => m.name.toLowerCase().includes('qwen') || m.name.toLowerCase().includes('llama'));
                    setSelectedModel(preferred ? preferred.name : models[0].name);
                } else {
                    // No models found - Update welcome message
                    setMessages([{
                        id: 'no-models',
                        role: 'assistant',
                        content: "âš ï¸ **No AI models detected.**\n\nTo start chatting, please go to the **Settings** page and download a model (like Qwen 2.5 or Llama 3). Once downloaded, refresh this page.",
                        timestamp: Date.now(),
                        type: 'text'
                    }]);
                }

                // LoRAs
                const loras = await comfyService.getLoras();
                setAvailableLoras(loras);

            } catch (error) {
                console.error('Failed to load data:', error);
            }
        };
        fetchData();
    }, []);

    const scrollToBottom = () => {
        messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
    };

    useEffect(() => {
        scrollToBottom();
    }, [messages, isThinking]);

    const handleSend = async () => {
        if (!input.trim()) return;

        const inputImages = [...pendingImages];

        const userMsg: Message = {
            id: Date.now().toString(),
            role: 'user',
            content: input,
            timestamp: Date.now(),
            type: 'text',
            images: inputImages.length > 0 ? inputImages : undefined
        };

        setMessages(prev => [...prev, userMsg]);
        setInput('');
        setPendingImages([]);
        setIsThinking(true);

        try {
            // Prepare history for Ollama
            // We keep the prompt content in history so the model knows what it generated previously
            const history = messages.map(m => ({
                role: m.role,
                content: m.content.replace(/<<GENERATE>>([\s\S]*?)(?:<<\/GENERATE>>|<<\/GENERATOR>>|$)/i, ' [Previous Prompt: $1] '),
                images: m.images ? m.images.map(img => img.replace(/^data:image\/[a-z]+;base64,/, "")) : undefined
            }));
            history.push({
                role: 'user',
                content: userMsg.content,
                images: inputImages.map(img => img.replace(/^data:image\/[a-z]+;base64,/, ""))
            });

            // Add system prompt at the start
            const fullHistory = [{ role: 'system', content: AGENT_SYSTEM_PROMPT }, ...history];

            // Use Ollama to chat
            // Use Ollama to chat with selected model
            const responseText = await assistantService.chat(selectedModel, fullHistory);

            // Determine if the user actually intended to generate something
            // (Prevents model from hallucinating cards on "Hi" or small talk)
            const generationKeywords = ['generate', 'create', 'make', 'draw', 'picture', 'image', 'photo', 'portrait', 'show', 'change', 'add', 'remove', 'variant', 'version', 'look like', 'lag', 'bilde', 'vis'];
            const userHasIntent = generationKeywords.some(kw => input.toLowerCase().includes(kw)) || input.length > 30;

            // Parse response for <<GENERATE>> (Robust)
            const genMatch = responseText.match(/<<GENERATE>>([\s\S]*?)(?:<<\/GENERATE>>|<<\/GENERATOR>>|$)/i);

            if (genMatch && userHasIntent) {
                const textPart = responseText.replace(/<<GENERATE>>[\s\S]*?(?:<<\/GENERATE>>|<<\/GENERATOR>>|$)/i, '').trim();
                let promptPart = genMatch[1].trim();

                // Cleanup: Remove leading "[Detailed Prompt]", strip quotes, and trim
                promptPart = promptPart.replace(/^\[Detailed Prompt\][\s:-]*/i, '');
                promptPart = promptPart.replace(/^["'â€œâ€]|["'â€œâ€]$/g, '').trim();

                // Add the generation request card ONLY if prompt is valid
                if (promptPart && promptPart.length > 5 && !['hi', 'hello', 'hey'].includes(promptPart.toLowerCase())) {
                    // Add text
                    if (textPart) {
                        setMessages(prev => [...prev, {
                            id: Date.now().toString(),
                            role: 'assistant',
                            content: textPart,
                            timestamp: Date.now(),
                            type: 'text'
                        }]);
                    }
                    // Add Card
                    setMessages(prev => [...prev, {
                        id: (Date.now() + 1).toString(),
                        role: 'assistant',
                        content: promptPart,
                        timestamp: Date.now(),
                        type: 'image-generation-request'
                    }]);
                } else {
                    // Malformed or empty prompt -> Treat as text
                    setMessages(prev => [...prev, {
                        id: Date.now().toString(),
                        role: 'assistant',
                        content: responseText.replace(/<<GENERATE>>[\s\S]*?(?:<<\/GENERATE>>|<<\/GENERATOR>>|$)/i, '').trim() || responseText,
                        timestamp: Date.now(),
                        type: 'text'
                    }]);
                }

            } else {
                // Normal text response (strip hallucinated tags if they shouldn't be there)
                const cleanText = responseText.replace(/<<GENERATE>>[\s\S]*?(?:<<\/GENERATE>>|<<\/GENERATOR>>|$)/i, '').trim();

                setMessages(prev => [...prev, {
                    id: Date.now().toString(),
                    role: 'assistant',
                    content: cleanText || responseText, // Fallback to raw if cleaning emptied it entirely
                    timestamp: Date.now(),
                    type: 'text'
                }]);
            }

        } catch (error) {
            console.error(error);
            setMessages(prev => [...prev, {
                id: Date.now().toString(),
                role: 'assistant',
                content: "Sorry, I had trouble connecting to the brain (Ollama). Is it running?",
                timestamp: Date.now(),
                type: 'text'
            }]);
        } finally {
            setIsThinking(false);
        }
    };

    const handleGenerateImage = async (msgId: string, prompt: string) => {
        setGeneratingMsgId(msgId);

        try {
            // Free VRAM from Ollama before ComfyUI generation
            if (selectedModel) {
                await ollamaService.unloadModel(selectedModel);
            }

            // Load the same Z-Image workflow as ImagePage
            const response = await fetch('/workflows/z-image.json');
            if (!response.ok) throw new Error('Failed to load Z-Image workflow');
            const workflow = await response.json();

            // Generate random seed
            const seed = Math.floor(Math.random() * 1000000000);

            console.log('ðŸ¤– Agent generating image:', { prompt, seed });

            // Modify workflow parameters (same node IDs as ImagePage)
            if (workflow["3"]) {
                workflow["3"].inputs.seed = seed;
                workflow["3"].inputs.steps = 9;
                workflow["3"].inputs.cfg = 1;
            }

            // Node 33: Positive Prompt
            if (workflow["33"]) {
                workflow["33"].inputs.string = prompt;
            }

            // Node 34: Negative Prompt
            if (workflow["34"]) {
                workflow["34"].inputs.string = "text, watermark, blur, ugly";
            }

            // Node 30: Dimensions (1024x1024 default for chat)
            if (workflow["30"]) {
                workflow["30"].inputs.width = 1024;
                workflow["30"].inputs.height = 1024;
            }

            // Node 126: LoRA Injection
            if (workflow["126"]) {
                if (selectedLora) {
                    workflow["126"].inputs.lora_1 = {
                        "on": true,
                        "lora": selectedLora,
                        "strength": 1.0
                    };
                    console.log('ðŸŽ¨ Applying LoRA:', selectedLora);
                } else {
                    // Disable LoRA if none selected
                    if (workflow["126"].inputs.lora_1) {
                        workflow["126"].inputs.lora_1.on = false;
                    }
                }
            }

            // Node 3: Randomize Seed (Critical for variations)
            if (workflow["3"]) {
                const randomSeed = Math.floor(Math.random() * 1000000000000000); // 15 digits
                workflow["3"].inputs.seed = randomSeed;
                console.log('ðŸŽ² New Seed generated:', randomSeed);
            }

            // 2. Queue the workflow and get prompt_id
            console.log('ðŸ“¤ Queueing workflow...');
            const { prompt_id } = await comfyService.queuePrompt(workflow);
            console.log('âœ… Queued! Prompt ID:', prompt_id);

            // 3. Setup WebSocket for progress tracking
            const cleanup = comfyService.connectWebSocket({
                onExecuting: (nodeId) => {
                    if (!nodeId) {
                        setExecutionStatus('Finalizing...');
                        return;
                    }
                    const statusMap: Record<string, string> = {
                        '3': 'Generating Image (Sampling)...',
                        '126': 'Loading LoRAs...',
                        '9': 'Saving Image...',
                    };
                    setExecutionStatus(statusMap[nodeId] || `Processing Node ${nodeId}...`);
                },
                onProgress: (_node, value, max) => {
                    setProgress(Math.round((value / max) * 100));
                }
            });

            // 4. Poll for completion and fetch image
            let attempts = 0;
            const maxAttempts = 60; // Wait up to 120s (60 * 2s) for generation
            let imageUrl: string | null = null;

            while (attempts < maxAttempts && !imageUrl) {
                attempts++;
                await new Promise(r => setTimeout(r, 2000)); // Wait 2 seconds between attempts

                console.log(`ðŸ“š Fetching history (attempt ${attempts}/${maxAttempts})...`);

                try {
                    const history = await comfyService.getHistory(prompt_id);

                    if (history[prompt_id] && history[prompt_id].outputs) {
                        const outputs = history[prompt_id].outputs['9'];

                        if (outputs && outputs.images && outputs.images.length > 0) {
                            const img = outputs.images[0];
                            // Add a cache-buster timestamp to ensure fresh image is displayed
                            imageUrl = `${comfyService.getImageUrl(img.filename, img.subfolder, img.type)}&t=${Date.now()}`;
                            console.log('âœ… Fresh Image found!', imageUrl);
                        }
                    }
                } catch (err) {
                    console.warn(`âš ï¸ Attempt ${attempts} failed:`, err);
                }
            }

            cleanup();

            if (!imageUrl) {
                throw new Error('Failed to retrieve image after all attempts');
            }

            // 4. Add Image Result Message
            setMessages(prev => [...prev, {
                id: Date.now().toString(),
                role: 'assistant',
                content: prompt, // Keep prompt as content or reference
                type: 'image-result',
                timestamp: Date.now(),
                metadata: { imageUrl }
            }]);

        } catch (error) {
            console.error("Gen Error:", error);
            setMessages(prev => [...prev, {
                id: Date.now().toString(),
                role: 'assistant',
                content: "Something went wrong sending the image to ComfyUI.",
                timestamp: Date.now(),
                type: 'text'
            }]);
        } finally {
            setGeneratingMsgId(null);
            setExecutionStatus('');
            setProgress(0);
        }
    };

    // Drag & Drop Handlers
    const handleDragOver = (e: React.DragEvent) => {
        e.preventDefault();
        setIsDragging(true);
    };

    const handleDragLeave = (e: React.DragEvent) => {
        e.preventDefault();
        setIsDragging(false);
    };

    const handleDrop = async (e: React.DragEvent) => {
        e.preventDefault();
        setIsDragging(false);

        const files = Array.from(e.dataTransfer.files);
        const imageFiles = files.filter(f => f.type.startsWith('image/'));

        if (imageFiles.length === 0) return;

        const base64Promises = imageFiles.map(file => {
            return new Promise<string>((resolve, reject) => {
                const reader = new FileReader();
                reader.onload = () => {
                    resolve(reader.result as string);
                };
                reader.onerror = reject;
                reader.readAsDataURL(file);
            });
        });

        try {
            const base64Images = await Promise.all(base64Promises);
            setPendingImages(prev => [...prev, ...base64Images]);
        } catch (error) {
            console.error('Failed to process dropped images:', error);
        }
    };

    const handleKeyDown = (e: React.KeyboardEvent) => {
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleSend();
        }
    };

    // Voice Recording Functions
    const startRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

            // Use webm/opus for best compatibility and smallest size
            const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
                ? 'audio/webm;codecs=opus'
                : 'audio/webm';

            const mediaRecorder = new MediaRecorder(stream, { mimeType });
            mediaRecorderRef.current = mediaRecorder;
            audioChunksRef.current = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunksRef.current.push(event.data);
                }
            };

            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });
                stream.getTracks().forEach(track => track.stop());
                await transcribeAudio(audioBlob);
            };

            mediaRecorder.start();
            setIsRecording(true);

            // Auto-stop after 30 seconds
            recordingTimeoutRef.current = setTimeout(() => {
                if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
                    stopRecording();
                }
            }, 30000);

        } catch (error) {
            console.error('Error accessing microphone:', error);
            setMessages(prev => [...prev, {
                id: Date.now().toString(),
                role: 'assistant',
                content: 'âš ï¸ Could not access microphone. Please check permissions.',
                timestamp: Date.now(),
                type: 'text'
            }]);
        }
    };

    const stopRecording = () => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            mediaRecorderRef.current.stop();
            setIsRecording(false);

            if (recordingTimeoutRef.current) {
                clearTimeout(recordingTimeoutRef.current);
                recordingTimeoutRef.current = null;
            }
        }
    };

    const transcribeAudio = async (audioBlob: Blob) => {
        setIsTranscribing(true);
        try {
            const formData = new FormData();
            formData.append('audio', audioBlob, 'recording.webm');

            const response = await fetch('/api/audio/transcribe', {
                method: 'POST',
                body: formData
            });

            if (!response.ok) {
                throw new Error('Transcription failed');
            }

            const data = await response.json();
            const transcribedText = data.text || '';

            if (transcribedText.trim()) {
                // Option C from questions: both flows available
                // For now, set in input field so user can edit before sending
                setInput(prev => prev + (prev ? ' ' : '') + transcribedText);
            } else {
                throw new Error('No speech detected');
            }

        } catch (error) {
            console.error('Transcription error:', error);
            setMessages(prev => [...prev, {
                id: Date.now().toString(),
                role: 'assistant',
                content: 'âš ï¸ Could not transcribe audio. Please try again.',
                timestamp: Date.now(),
                type: 'text'
            }]);
        } finally {
            setIsTranscribing(false);
        }
    };

    const handleMicMouseDown = () => {
        if (recordingMode === 'hold') {
            startRecording();
        }
    };

    const handleMicMouseUp = () => {
        if (recordingMode === 'hold' && isRecording) {
            stopRecording();
        }
    };

    const handleMicClick = () => {
        if (recordingMode === 'toggle') {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        }
    };

    // TTS Functions
    const playTTS = async (messageId: string, text: string) => {
        try {
            setGeneratingTtsId(messageId);

            // Generate TTS
            const response = await fetch('/api/audio/tts', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    text,
                    voice_style: voiceStyle
                })
            });

            if (!response.ok) throw new Error('TTS generation failed');

            // Get audio blob
            const audioBlob = await response.blob();
            const audioUrl = URL.createObjectURL(audioBlob);

            // Stop any currently playing audio
            if (audioPlayerRef.current) {
                audioPlayerRef.current.pause();
                audioPlayerRef.current = null;
            }

            // Create and play audio
            const audio = new Audio(audioUrl);
            audioPlayerRef.current = audio;
            setPlayingMsgId(messageId);
            setGeneratingTtsId(null);

            audio.onended = () => {
                setPlayingMsgId(null);
                URL.revokeObjectURL(audioUrl);
            };

            audio.onerror = () => {
                setPlayingMsgId(null);
                setGeneratingTtsId(null);
                URL.revokeObjectURL(audioUrl);
            };

            await audio.play();

        } catch (error) {
            console.error('TTS error:', error);
            setPlayingMsgId(null);
            setGeneratingTtsId(null);
        }
    };

    const stopTTS = () => {
        if (audioPlayerRef.current) {
            audioPlayerRef.current.pause();
            audioPlayerRef.current = null;
        }
        setPlayingMsgId(null);
    };

    // Auto-play TTS for new assistant messages when enabled
    useEffect(() => {
        if (!ttsEnabled || messages.length === 0) return;

        const lastMessage = messages[messages.length - 1];
        if (lastMessage.role === 'assistant' && lastMessage.type === 'text' && !playingMsgId && !generatingTtsId) {
            // Auto-play with a slight delay to feel natural
            setTimeout(() => {
                playTTS(lastMessage.id, lastMessage.content);
            }, 300);
        }
    }, [messages, ttsEnabled]);

    return (
        <div
            className="flex flex-col h-full w-full relative"
            onDragOver={handleDragOver}
            onDragLeave={handleDragLeave}
            onDrop={handleDrop}
        >
            {/* Drag Overlay */}
            {isDragging && (
                <div className="absolute inset-0 bg-black/80 z-50 flex items-center justify-center border-4 border-dashed border-white/30 rounded-xl backdrop-blur-sm pointer-events-none animate-in fade-in duration-200">
                    <div className="flex flex-col items-center gap-4 animate-bounce">
                        <ImageIcon className="w-16 h-16 text-white" />
                        <span className="text-2xl font-bold text-white tracking-widest uppercase">Drop Idea Here</span>
                    </div>
                </div>
            )}

            {/* Header */}
            <div className="absolute top-0 left-0 right-0 p-6 z-10 bg-gradient-to-b from-[#050508] to-transparent">
                <div className="flex items-center justify-between">
                    <div className="flex items-center gap-4">
                        <div className="flex items-center gap-3 opacity-50 pointer-events-none">
                            <Bot className="w-5 h-5" />
                            <span className="text-sm font-medium tracking-wider uppercase">Fedda Agent</span>
                        </div>

                        {/* Brain Selector */}
                        <div className="flex items-center gap-2 bg-[#121218] border border-white/10 rounded-lg px-2 py-1 z-20 pointer-events-auto shadow-lg">
                            <span className="text-xs text-slate-500">Brain:</span>
                            <select
                                value={selectedModel}
                                onChange={(e) => setSelectedModel(e.target.value)}
                                className="bg-transparent text-xs text-white border-none focus:ring-0 cursor-pointer outline-none"
                                disabled={availableModels.length === 0}
                            >
                                {availableModels.length === 0 ? (
                                    <option value="">No models found</option>
                                ) : (
                                    availableModels.map(m => (
                                        <option key={m} value={m} className="bg-[#121218] text-white py-1">{m}</option>
                                    ))
                                )}
                            </select>
                        </div>
                    </div>

                    {/* Font Size Control */}
                    <div className="flex items-center gap-2 pointer-events-auto">

                        {/* LoRA Selector */}
                        <div className="flex items-center gap-2 bg-[#121218] border border-white/10 rounded-lg px-2 py-1 mr-2">
                            <span className="text-xs text-slate-500">Style:</span>
                            <select
                                value={selectedLora}
                                onChange={(e) => setSelectedLora(e.target.value)}
                                className="bg-transparent text-xs text-white border-none focus:ring-0 cursor-pointer outline-none max-w-[120px]"
                                disabled={availableLoras.length === 0}
                            >
                                <option value="">{availableLoras.length === 0 ? 'None (No LoRAs found)' : 'None'}</option>
                                {availableLoras.map(l => (
                                    <option key={l} value={l} className="bg-[#121218]">{l.replace('.safetensors', '').replace('.pt', '')}</option>
                                ))}
                            </select>
                        </div>

                        <Type className="w-4 h-4 text-slate-500" />
                        <div className="flex gap-1 bg-[#121218] border border-white/10 rounded-lg p-1">
                            {(['small', 'medium', 'large'] as const).map((size) => (
                                <button
                                    key={size}
                                    onClick={() => setFontSize(size)}
                                    className={`px-2 py-1 rounded text-xs font-medium transition-all ${fontSize === size
                                        ? 'bg-white text-black'
                                        : 'text-slate-400 hover:text-white'
                                        }`}
                                >
                                    {size === 'small' ? 'S' : size === 'medium' ? 'M' : 'L'}
                                </button>
                            ))}
                        </div>

                        {/* TTS Controls */}
                        <div className="flex items-center gap-3">
                            <button
                                onClick={() => setTtsEnabled(!ttsEnabled)}
                                className={`flex items-center gap-2 px-3 py-1.5 rounded-lg transition-all ${ttsEnabled
                                    ? 'bg-white/10 text-white'
                                    : 'bg-white/5 text-slate-400 hover:text-white'
                                    }`}
                                title="Toggle AI voice responses"
                            >
                                {ttsEnabled ? <Volume2 className="w-4 h-4" /> : <VolumeX className="w-4 h-4" />}
                                <span className="text-xs font-medium">Voice</span>
                            </button>

                            {/* Voice Style Selector */}
                            {ttsEnabled && (
                                <select
                                    value={voiceStyle}
                                    onChange={(e) => setVoiceStyle(e.target.value)}
                                    className="px-2 py-1 rounded-lg bg-[#121218] border border-white/10 text-xs text-white"
                                >
                                    <option value="female, clear voice">Female</option>
                                    <option value="man with low pitch tembre">Male Deep</option>
                                    <option value="cheerful woman">Cheerful</option>
                                    <option value="professional male narrator">Professional</option>
                                </select>
                            )}
                        </div>
                    </div>
                </div>
            </div>

            {/* Messages Area */}
            <div className="flex-1 overflow-y-auto px-12 py-20 space-y-8 custom-scrollbar">
                {messages.map((msg) => (
                    <div
                        key={msg.id}
                        className={`flex gap-4 ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
                    >
                        {msg.role === 'assistant' && (
                            <div className="w-8 h-8 rounded-full bg-white/10 flex items-center justify-center flex-shrink-0 mt-1 shadow-inner">
                                <Bot className="w-4 h-4 text-white" />
                            </div>
                        )}

                        <div className={`max-w-[95%] flex flex-col gap-2 ${msg.role === 'user' ? 'items-end' : 'items-start'}`}>

                            {/* User Images */}
                            {msg.images && msg.images.length > 0 && (
                                <div className={`flex flex-wrap gap-2 mb-2 ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}>
                                    {msg.images.map((img, idx) => (
                                        <img key={idx} src={img} alt="Context" className="w-32 h-32 object-cover rounded-xl border border-white/20 shadow-lg" />
                                    ))}
                                </div>
                            )}

                            {/* Text Bubble */}
                            {msg.type === 'text' && (
                                <div className={`px-5 py-4 rounded-2xl ${msg.role === 'user'
                                    ? 'bg-white text-black shadow-[0_0_20px_rgba(255,255,255,0.1)]'
                                    : 'bg-[#121218] border border-white/5 text-slate-200 shadow-xl'
                                    }`}>
                                    <div className={`prose prose-invert prose-sm max-w-none ${fontSizeClasses[fontSize]}`}>
                                        <ReactMarkdown>
                                            {msg.content}
                                        </ReactMarkdown>
                                    </div>
                                </div>
                            )}

                            {/* TTS Button for Assistant Messages */}
                            {msg.role === 'assistant' && msg.type === 'text' && (
                                <button
                                    onClick={() => {
                                        if (playingMsgId === msg.id) {
                                            stopTTS();
                                        } else {
                                            playTTS(msg.id, msg.content);
                                        }
                                    }}
                                    disabled={generatingTtsId === msg.id}
                                    className="flex items-center gap-2 px-3 py-1.5 rounded-lg bg-white/5 hover:bg-white/10 transition-colors text-xs text-white/50 hover:text-white/70 disabled:opacity-50"
                                    title={playingMsgId === msg.id ? "Stop" : "Play voice"}
                                >
                                    {generatingTtsId === msg.id ? (
                                        <>
                                            <Loader2 className="w-3 h-3 animate-spin" />
                                            <span>Generating...</span>
                                        </>
                                    ) : playingMsgId === msg.id ? (
                                        <>
                                            <VolumeX className="w-3 h-3" />
                                            <span>Stop</span>
                                        </>
                                    ) : (
                                        <>
                                            <Volume2 className="w-3 h-3" />
                                            <span>Play</span>
                                        </>
                                    )}
                                </button>
                            )}

                            {/* Generation Request Card */}
                            {msg.type === 'image-generation-request' && (
                                <div className="bg-[#121218] border border-white/10 rounded-2xl p-5 shadow-2xl w-full max-w-2xl animate-in zoom-in-95 duration-300">
                                    <div className="flex items-center gap-2 mb-3 text-white/50 text-xs font-bold tracking-wider uppercase">
                                        <Sparkles className="w-3 h-3" />
                                        <span>Ready to Generate</span>
                                    </div>
                                    <p className="text-sm text-slate-300 italic mb-4 border-l-2 border-white/20 pl-3 py-1">
                                        "{msg.content}"
                                    </p>

                                    {/* Progress Indicator */}
                                    {generatingMsgId === msg.id && executionStatus && (
                                        <div className="mb-4 space-y-2">
                                            <div className="flex items-center justify-between text-xs">
                                                <span className="text-slate-400">{executionStatus}</span>
                                                <span className="text-white font-mono">{progress}%</span>
                                            </div>
                                            <div className="w-full h-1.5 bg-white/10 rounded-full overflow-hidden">
                                                <div
                                                    className="h-full bg-white transition-all duration-300 ease-out"
                                                    style={{ width: `${progress}%` }}
                                                />
                                            </div>
                                        </div>
                                    )}

                                    <button
                                        onClick={() => handleGenerateImage(msg.id, msg.content)}
                                        disabled={generatingMsgId !== null}
                                        className="w-full bg-white hover:bg-slate-200 text-black py-3 rounded-xl font-bold transition-all shadow-lg flex items-center justify-center gap-2"
                                    >
                                        {generatingMsgId === msg.id ? (
                                            <>
                                                <Loader2 className="w-4 h-4 animate-spin" />
                                                Generating...
                                            </>
                                        ) : (
                                            <>
                                                <ImageIcon className="w-4 h-4" />
                                                Generate Image
                                            </>
                                        )}
                                    </button>
                                </div>
                            )}

                            {/* Image Result Card */}
                            {msg.type === 'image-result' && (
                                <div className="bg-[#121218] border border-white/10 rounded-2xl p-2 shadow-2xl overflow-hidden animate-in zoom-in-95 duration-500">
                                    <img
                                        src={msg.metadata?.imageUrl}
                                        alt="Generated"
                                        className="rounded-xl w-full h-auto object-cover max-h-[800px]"
                                    />
                                    <div className="p-3 flex items-center justify-between">
                                        <span className="text-xs text-slate-500 font-mono">Z-Image v1</span>
                                        <button className="text-xs text-white hover:underline">Download</button>
                                    </div>
                                </div>
                            )}

                        </div>

                        {msg.role === 'user' && (
                            <div className="w-8 h-8 rounded-full bg-white flex items-center justify-center flex-shrink-0 mt-1 shadow-[0_0_15px_rgba(255,255,255,0.3)]">
                                <User className="w-4 h-4 text-black" />
                            </div>
                        )}
                    </div>
                ))}

                {isThinking && (
                    <div className="flex gap-4">
                        <div className="w-8 h-8 rounded-full bg-white/10 flex items-center justify-center flex-shrink-0 mt-1">
                            <Bot className="w-4 h-4 text-white" />
                        </div>
                        <div className="bg-[#121218] border border-white/5 rounded-2xl p-4 flex items-center gap-3">
                            <div className="flex gap-1">
                                <div className="w-1.5 h-1.5 bg-white rounded-full animate-bounce [animation-delay:-0.3s]"></div>
                                <div className="w-1.5 h-1.5 bg-white rounded-full animate-bounce [animation-delay:-0.15s]"></div>
                                <div className="w-1.5 h-1.5 bg-white rounded-full animate-bounce"></div>
                            </div>
                        </div>
                    </div>
                )}
                <div ref={messagesEndRef} />
            </div>

            {/* Input Area */}
            <div className="p-6 mb-4 relative z-20">
                <div className="relative w-full">

                    {/* Pending Image Preview */}
                    {pendingImages.length > 0 && (
                        <div className="absolute -top-24 left-0 flex gap-4 p-4 z-10">
                            {pendingImages.map((img, idx) => (
                                <div key={idx} className="relative group animate-in zoom-in-50 duration-200">
                                    <img src={img} className="w-20 h-20 object-cover rounded-lg border-2 border-white/30 shadow-2xl" />
                                    <button
                                        onClick={() => setPendingImages(prev => prev.filter((_, i) => i !== idx))}
                                        className="absolute -top-2 -right-2 bg-red-500 rounded-full w-5 h-5 flex items-center justify-center text-white text-xs font-bold shadow-md hover:bg-red-600 transition-colors"
                                    >
                                        âœ•
                                    </button>
                                </div>
                            ))}
                        </div>
                    )}

                    {/* Mode Toggle Tooltip */}
                    {isRecording && (
                        <div className="absolute -top-8 left-1/2 transform -translate-x-1/2 bg-red-500 text-white text-xs px-3 py-1 rounded-full font-medium animate-pulse">
                            Recording... ({recordingMode === 'hold' ? 'Release to stop' : 'Click to stop'})
                        </div>
                    )}

                    <textarea
                        value={input}
                        onChange={(e) => setInput(e.target.value)}
                        onKeyDown={handleKeyDown}
                        placeholder="Message Fedda Agent..."
                        disabled={isRecording || isTranscribing}
                        className="w-full bg-[#121218] border border-white/10 rounded-2xl pl-6 pr-28 py-4 text-sm text-slate-200 placeholder:text-slate-600 focus:outline-none focus:ring-2 focus:ring-white/20 resize-none shadow-2xl custom-scrollbar disabled:opacity-50"
                        style={{ minHeight: '60px', maxHeight: '200px' }}
                    />

                    {/* Microphone Button */}
                    <button
                        onMouseDown={handleMicMouseDown}
                        onMouseUp={handleMicMouseUp}
                        onMouseLeave={handleMicMouseUp}
                        onClick={handleMicClick}
                        disabled={isTranscribing}
                        className={`absolute right-16 bottom-3 p-2 rounded-xl transition-all shadow-lg disabled:opacity-50 disabled:cursor-not-allowed ${isRecording
                            ? 'bg-red-500 text-white animate-pulse'
                            : 'bg-slate-700 text-white hover:bg-slate-600'
                            }`}
                        title={recordingMode === 'hold' ? 'Hold to record' : 'Click to start/stop'}
                    >
                        {isTranscribing ? (
                            <Loader2 className="w-4 h-4 animate-spin" />
                        ) : isRecording ? (
                            <Square className="w-4 h-4" />
                        ) : (
                            <Mic className="w-4 h-4" />
                        )}
                    </button>

                    {/* Mode Toggle (small button) */}
                    <button
                        onClick={() => setRecordingMode(mode => mode === 'hold' ? 'toggle' : 'hold')}
                        disabled={isRecording || isTranscribing}
                        className="absolute right-16 bottom-[-20px] text-[10px] text-slate-500 hover:text-white transition-all disabled:opacity-50"
                        title="Toggle recording mode"
                    >
                        {recordingMode === 'hold' ? 'ðŸ“Œ Hold' : 'ðŸ”˜ Click'}
                    </button>

                    {/* Send Button */}
                    <button
                        onClick={handleSend}
                        disabled={!input.trim() || isThinking || isRecording || isTranscribing}
                        className="absolute right-3 bottom-3 p-2 bg-white text-black rounded-xl hover:bg-slate-200 disabled:opacity-50 disabled:cursor-not-allowed transition-all shadow-lg"
                    >
                        <Send className="w-4 h-4" />
                    </button>
                </div>
            </div>
        </div>
    );
};
